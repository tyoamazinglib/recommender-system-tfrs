{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub_IS986jE5Z"
      },
      "source": [
        "# Preparing Federated Dependency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejWQaeiAi8_a",
        "outputId": "2a0cf73d-3763-4095-a219-09ca8edb713a"
      },
      "outputs": [],
      "source": [
        "# Installing TensorFlow Federated\n",
        "!pip install --quiet tensorflow-federated\n",
        "# !pip install --quiet tensorflow_recommenders\n",
        "# !pip install --quiet nest-asyncio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wukz-jtIjLcq"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow_federated'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_federated\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtff\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_recommenders\u001b[39;00m \u001b[39mimport\u001b[39;00m layers\n\u001b[0;32m     21\u001b[0m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mseed(\u001b[39m42\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_federated'"
          ]
        }
      ],
      "source": [
        "# Import other stuff\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import collections\n",
        "import functools\n",
        "import io\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "from typing import List, Optional, Tuple, Union, Sequence\n",
        "\n",
        "import tensorflow_recommenders as tfrs\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "from tensorflow_recommenders import layers\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpGTPoOpC1b_"
      },
      "source": [
        "# Dataset preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_movielens_data(dataset_path):\n",
        "  \"\"\"Downloads and copies MovieLens data to local /tmp directory.\"\"\"\n",
        "  if dataset_path.startswith('http'):\n",
        "    r = requests.get(dataset_path)\n",
        "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "    z.extractall(path='/tmp')\n",
        "  else:\n",
        "    tf.io.gfile.makedirs('/tmp/ml-1m/')\n",
        "    for filename in ['ratings.dat', 'movies.dat', 'users.dat']:\n",
        "      tf.io.gfile.copy(\n",
        "          os.path.join(dataset_path, filename),\n",
        "          os.path.join('/tmp/ml-1m/', filename),\n",
        "          overwrite=True)\n",
        "\n",
        "download_movielens_data('http://files.grouplens.org/datasets/movielens/ml-1m.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPHzcTCPGYwb"
      },
      "outputs": [],
      "source": [
        "# Datasets download and preparation\n",
        "DATA_DIR = \"/tmp\"\n",
        "\n",
        "# Loads up the dataset using pandas\n",
        "ratings_df = pd.read_csv(os.path.join(DATA_DIR, \"ml-1m\", \"ratings.dat\"), \n",
        "                         sep='::', \n",
        "                         names=[\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"],\n",
        "                         engine=\"python\",\n",
        "                         encoding='latin-1')\n",
        "\n",
        "movies_df = pd.read_csv(os.path.join(DATA_DIR, \"ml-1m\", \"movies.dat\"), \n",
        "                         sep='::', \n",
        "                         names=[\"MovieID\", \"Title\", \"Genres\"],\n",
        "                         engine=\"python\",\n",
        "                         encoding='latin-1')\n",
        "\n",
        "# Mapping dictionary in a new index\n",
        "movie_mapping = {old_movie: new_movie for new_movie, old_movie in enumerate(ratings_df.MovieID.astype(\"category\").cat.categories)}\n",
        "user_mapping = {old_user: new_user for new_user, old_user in enumerate(ratings_df.UserID.astype(\"category\").cat.categories)}\n",
        "\n",
        "# Map each DataFrame consistently using the now-fixed mapping.\n",
        "ratings_df.MovieID = ratings_df.MovieID.map(movie_mapping)\n",
        "ratings_df.UserID = ratings_df.UserID.map(user_mapping)\n",
        "movies_df.MovieID = movies_df.MovieID.map(movie_mapping)\n",
        "\n",
        "# movies = movies_df.MovieID.map(movie_mapping)\n",
        "\n",
        "# Remove nulls result from movies_df\n",
        "movies_df = movies_df[pd.notnull(movies_df.MovieID)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehoE8FNTIg1B",
        "outputId": "7846383f-f86e-4890-f3e2-e526f166ad3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num users: 6040\n",
            "Num movies: 3706\n"
          ]
        }
      ],
      "source": [
        "# Sanity check of number of users and movies\n",
        "unique_movie = np.unique(np.concatenate([list(ratings_df.MovieID)])).astype(str)\n",
        "unique_ids = np.unique(np.concatenate([list(ratings_df.UserID)])).astype(str)\n",
        "print('Num users:', unique_ids.shape[0])\n",
        "print('Num movies:', unique_movie.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiGIJ6UmzWte"
      },
      "outputs": [],
      "source": [
        "# Define Embedding Dimension (arbitrary)\n",
        "embedding_dimension = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWGxF_fDy_UP"
      },
      "outputs": [],
      "source": [
        "# Modelling for 'candidate'\n",
        "candidate_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.StringLookup(vocabulary=unique_movie, mask_token=None),\n",
        "    tf.keras.layers.Embedding(len(unique_movie) + 1, embedding_dimension),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hM__VTXQ5okj"
      },
      "outputs": [],
      "source": [
        "class FactorizedTopK(tf.keras.layers.Layer):\n",
        "  \"\"\"Computes metrics for across top K candidates surfaced by a retrieval model.\n",
        "  The default metric is top K categorical accuracy: how often the true candidate\n",
        "   is in the top K candidates for a given query.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      candidates: Union[layers.factorized_top_k.TopK, tf.data.Dataset],\n",
        "      ks: Sequence[int] = (1, 5, 10, 50, 100),\n",
        "      name: str = \"factorized_top_k\",\n",
        "  ) -> None:\n",
        "    \"\"\"Initializes the metric.\n",
        "    Args:\n",
        "      candidates: A layer for retrieving top candidates in response\n",
        "        to a query, or a dataset of candidate embeddings from which\n",
        "        candidates should be retrieved.\n",
        "      ks: A sequence of values of `k` at which to perform retrieval evaluation.\n",
        "      name: Optional name.\n",
        "    \"\"\"\n",
        "\n",
        "    super().__init__(name=name)\n",
        "\n",
        "    if isinstance(candidates, tf.data.Dataset):\n",
        "      candidates = (\n",
        "          layers.factorized_top_k.Streaming(k=max(ks))\n",
        "          .index_from_dataset(candidates)\n",
        "      )\n",
        "\n",
        "    self._ks = ks\n",
        "    self._candidates = candidates\n",
        "    self._top_k_metrics = [\n",
        "        tf.keras.metrics.Mean(\n",
        "            name=f\"{self.name}/top_{x}_categorical_accuracy\"\n",
        "        ) for x in ks\n",
        "    ]\n",
        "\n",
        "  def update_state(\n",
        "      self,\n",
        "      query_embeddings: tf.Tensor,\n",
        "      true_candidate_embeddings: tf.Tensor,\n",
        "      true_candidate_ids: Optional[tf.Tensor] = None\n",
        "  ) -> tf.Operation:\n",
        "    \"\"\"Updates the metrics.\n",
        "    Args:\n",
        "      query_embeddings: [num_queries, embedding_dim] tensor of query embeddings.\n",
        "      true_candidate_embeddings: [num_queries, embedding_dim] tensor of\n",
        "        embeddings for candidates that were selected for the query.\n",
        "      true_candidate_ids: Ids of the true candidates. If supplied, evaluation\n",
        "        will be id-based: the supplied ids will be matched against the ids of\n",
        "        the top candidates returned from the retrieval index, which should have\n",
        "        been constructed with the appropriate identifiers.\n",
        "        If not supplied, evaluation will be score-based: the score of the true\n",
        "        candidate will be computed and compared with the scores returned from\n",
        "        the index for the top candidates.\n",
        "        Score-based evaluation is useful for when the true candidate is not\n",
        "        in the retrieval index. Id-based evaluation is useful for when scores\n",
        "        returned from the index are not directly comparable to scores computed\n",
        "        by multiplying the candidate and embedding vector. For example, scores\n",
        "        returned by ScaNN are quantized, and cannot be compared to\n",
        "        full-precision scores.\n",
        "    Returns:\n",
        "      Update op. Only used in graph mode.\n",
        "    \"\"\"\n",
        "\n",
        "    if true_candidate_ids is None and not self._candidates.is_exact():\n",
        "      raise ValueError(\n",
        "          f\"The candidate generation layer ({self._candidates}) does not return \"\n",
        "          \"exact results. To perform evaluation using that layer, you must \"\n",
        "          \"supply `true_candidate_ids`, which will be checked against \"\n",
        "          \"the candidate ids returned from the candidate generation layer.\"\n",
        "      )\n",
        "\n",
        "    positive_scores = tf.reduce_sum(\n",
        "        query_embeddings * true_candidate_embeddings, axis=1, keepdims=True)\n",
        "\n",
        "    top_k_predictions, retrieved_ids = self._candidates(\n",
        "        query_embeddings, k=max(self._ks))\n",
        "\n",
        "    update_ops = []\n",
        "\n",
        "    if true_candidate_ids is not None:\n",
        "      # We're using ID-based evaluation.\n",
        "      if len(true_candidate_ids.shape) == 1:\n",
        "        true_candidate_ids = tf.expand_dims(true_candidate_ids, 1)\n",
        "\n",
        "      # Deal with ScaNN using `NaN`-padding by converting its\n",
        "      # `NaN` scores into minimum scores.\n",
        "      nan_padding = tf.math.is_nan(top_k_predictions)\n",
        "      top_k_predictions = tf.where(\n",
        "          nan_padding,\n",
        "          tf.ones_like(top_k_predictions) * tf.float32.min,\n",
        "          top_k_predictions\n",
        "      )\n",
        "\n",
        "      # Check sortedness.\n",
        "      is_sorted = (\n",
        "          top_k_predictions[:, :-1] - top_k_predictions[:, 1:]\n",
        "      )\n",
        "      tf.debugging.assert_non_negative(\n",
        "          is_sorted, message=\"Top-K predictions must be sorted.\"\n",
        "      )\n",
        "\n",
        "      # Check whether the true candidates were retrieved, accounting\n",
        "      # for padding.\n",
        "      ids_match = tf.cast(\n",
        "          tf.math.logical_and(\n",
        "              tf.math.equal(true_candidate_ids, retrieved_ids),\n",
        "              tf.math.logical_not(nan_padding)\n",
        "          ),\n",
        "          tf.float32\n",
        "      )\n",
        "\n",
        "      for k, metric in zip(self._ks, self._top_k_metrics):\n",
        "        # By slicing until :k we assume scores are sorted.\n",
        "        # Clip to only count multiple matches once.\n",
        "        match_found = tf.clip_by_value(\n",
        "            tf.reduce_sum(ids_match[:, :k], axis=1, keepdims=True),\n",
        "            0.0, 1.0\n",
        "        )\n",
        "        update_ops.append(metric.update_state(match_found))\n",
        "    else:\n",
        "      # Score-based evaluation.\n",
        "      y_pred = tf.concat([positive_scores, top_k_predictions], axis=1)\n",
        "\n",
        "      for k, metric in zip(self._ks, self._top_k_metrics):\n",
        "        targets = tf.zeros(tf.shape(positive_scores)[0], dtype=tf.int32)\n",
        "        top_k_accuracy = tf.math.in_top_k(\n",
        "            targets=targets,\n",
        "            predictions=y_pred,\n",
        "            k=k\n",
        "        )\n",
        "        update_ops.append(metric.update_state(top_k_accuracy))\n",
        "\n",
        "    return tf.group(update_ops)\n",
        "\n",
        "  def reset_states(self) -> None:\n",
        "    \"\"\"Resets the metrics.\"\"\"\n",
        "\n",
        "    for metric in self.metrics:\n",
        "      metric.reset_states()\n",
        "\n",
        "  def result(self) -> List[tf.Tensor]:\n",
        "    \"\"\"Returns a list of metric results.\"\"\"\n",
        "\n",
        "    return [metric.result() for metric in self.metrics]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upIfIQoHI4_X"
      },
      "source": [
        "# Preprocessing Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sm5JOFBVFpHG"
      },
      "outputs": [],
      "source": [
        "# Helper function to create dataset based on user list\n",
        "def create_tf_datasets(ratings_df: pd.DataFrame,\n",
        "                       batch_size: int = 1,\n",
        "                       max_examples_per_user: Optional[int] = None,\n",
        "                       max_clients: Optional[int] = None) -> List[tf.data.Dataset]:\n",
        "  num_users = len(set(ratings_df.UserID))\n",
        "  # Optionally limit to `max_clients` to speed up data loading.\n",
        "  if max_clients is not None:\n",
        "    num_users = min(num_users, max_clients)\n",
        "\n",
        "  def rating_batch_map_fn(rating_batch):\n",
        "    # Each example looks like: {x: movie_id, y: rating}.\n",
        "    # We won't need the UserID since each client will only look at their own\n",
        "    # data.\n",
        "    return collections.OrderedDict([\n",
        "        (\"x\", tf.cast(rating_batch[:, 1:2], tf.int64)),\n",
        "        (\"y\", tf.cast(rating_batch[:, 2:3], tf.float32))\n",
        "    ])\n",
        "\n",
        "  tf_datasets = []\n",
        "  for user in range(num_users):\n",
        "    # Get subset of ratings_df belonging to a particular user.\n",
        "    user_ratings_df = ratings_df[ratings_df.UserID == user]\n",
        "\n",
        "    tf_dataset = tf.data.Dataset.from_tensor_slices(user_ratings_df)\n",
        "\n",
        "    # Define preprocessing operations.\n",
        "    tf_dataset = tf_dataset.take(max_examples_per_user).shuffle(\n",
        "        buffer_size=max_examples_per_user, seed=42).batch(batch_size).map(\n",
        "        rating_batch_map_fn,\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    tf_datasets.append(tf_dataset)\n",
        "\n",
        "  return tf_datasets\n",
        "\n",
        "def split_tf_datasets(\n",
        "    tf_datasets: List[tf.data.Dataset],\n",
        "    train_fraction: float = 0.8,\n",
        "    val_fraction: float = 0.1,\n",
        ") -> Tuple[List[tf.data.Dataset], List[tf.data.Dataset], List[tf.data.Dataset]]:\n",
        "  np.random.seed(42)\n",
        "  np.random.shuffle(tf_datasets)\n",
        "\n",
        "  train_idx = int(len(tf_datasets) * train_fraction)\n",
        "  val_idx = int(len(tf_datasets) * (train_fraction + val_fraction))\n",
        "\n",
        "  # Note that the val and test data contains completely different users, not\n",
        "  # just unseen ratings from train users.\n",
        "  return (tf_datasets[:train_idx], tf_datasets[train_idx:val_idx],\n",
        "          tf_datasets[val_idx:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwdmRxZSF9xp"
      },
      "outputs": [],
      "source": [
        "# We limit the number of clients to speed up dataset creation. Feel free to pass\n",
        "# max_clients=None to load all clients' data.\n",
        "tf_datasets = create_tf_datasets(\n",
        "    ratings_df=ratings_df,\n",
        "    batch_size=5,\n",
        "    max_examples_per_user=300,\n",
        "    max_clients=2000)\n",
        "\n",
        "# Split the ratings into training/val/test by client.\n",
        "tf_train_datasets, tf_val_datasets, tf_test_datasets = split_tf_datasets(\n",
        "    tf_datasets,\n",
        "    train_fraction=0.8,\n",
        "    val_fraction=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFoVqG3JJizx"
      },
      "source": [
        "# Defining Federated Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHKW37p-H_mp"
      },
      "outputs": [],
      "source": [
        "# Define a single Keras layer representing an embedding for a single user\n",
        "class UserEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_latent_factors, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.num_latent_factors = num_latent_factors\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.embedding = self.add_weight(\n",
        "        shape=(1, self.num_latent_factors),\n",
        "        initializer='uniform',\n",
        "        dtype=tf.float32,\n",
        "        name='UserEmbeddingKernel')\n",
        "    super().build(input_shape)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return self.embedding\n",
        "\n",
        "  def compute_output_shape(self):\n",
        "    return (1, self.num_latent_factors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcH-rEHRJwMJ"
      },
      "outputs": [],
      "source": [
        "def get_matrix_factorization_model(\n",
        "    num_items: int,\n",
        "    num_latent_factors: int) -> tff.learning.reconstruction.Model:\n",
        "  \"\"\"Defines a Keras matrix factorization model.\"\"\"\n",
        "  # Layers with variables will be partitioned into global and local layers.\n",
        "  # We'll pass this to `tff.learning.reconstruction.from_keras_model`.\n",
        "  global_layers = []\n",
        "  local_layers = []\n",
        "\n",
        "  # Extract the item embedding.\n",
        "  # This will be the global layers and the layers stored on the centralized setting\n",
        "  item_input = tf.keras.layers.Input(shape=[1], name='Item')\n",
        "  item_embedding_layer = tf.keras.layers.Embedding(\n",
        "      num_items,\n",
        "      num_latent_factors,\n",
        "      name='ItemEmbedding')\n",
        "  global_layers.append(item_embedding_layer)\n",
        "  flat_item_vec = tf.keras.layers.Flatten(name='FlattenItems')(\n",
        "      item_embedding_layer(item_input))\n",
        "\n",
        "  # Extract the user embedding.\n",
        "  # This will be the local layers and the layers stored on each clients setting\n",
        "  user_embedding_layer = UserEmbedding(\n",
        "      num_latent_factors,\n",
        "      name='UserEmbedding')\n",
        "  local_layers.append(user_embedding_layer)\n",
        "\n",
        "  # The item_input never gets used by the user embedding layer,\n",
        "  # but this allows the model to directly use the user embedding.\n",
        "  flat_user_vec = user_embedding_layer(item_input)\n",
        "\n",
        "  # Compute the dot product between the user embedding, and the item one.\n",
        "  pred = tf.keras.layers.Dot(\n",
        "      1, normalize=False, name='Dot')([flat_user_vec, flat_item_vec])\n",
        "\n",
        "  input_spec = collections.OrderedDict(\n",
        "      x=tf.TensorSpec(shape=[None, 1], dtype=tf.int64),\n",
        "      y=tf.TensorSpec(shape=[None, 1], dtype=tf.float32))\n",
        "  \n",
        "  model = tf.keras.Model(inputs=item_input, outputs=pred)\n",
        "\n",
        "  return tff.learning.reconstruction.from_keras_model(\n",
        "      keras_model=model,\n",
        "      global_layers=global_layers,\n",
        "      local_layers=local_layers,\n",
        "      input_spec=input_spec)\n",
        "\n",
        "model_fn = functools.partial(\n",
        "    get_matrix_factorization_model,\n",
        "    num_items=len(set(ratings_df.MovieID)),\n",
        "    num_latent_factors=embedding_dimension)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 866
        },
        "id": "tM1esS0R6cWR",
        "outputId": "7889d8ed-ca70-4f54-cf8e-8883188fb3ae"
      },
      "outputs": [
        {
          "ename": "UnimplementedError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-855fc823b40c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m top_k_metrics = FactorizedTopK(\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mcandidates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmovies_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMovieID\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK_Factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, arg, na_action)\u001b[0m\n\u001b[1;32m   4235\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4236\u001b[0m         \"\"\"\n\u001b[0;32m-> 4237\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4238\u001b[0m         return self._constructor(new_values, index=self.index).__finalize__(\n\u001b[1;32m   4239\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"map\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0;31m# mapper is a function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7213\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7214\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7215\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnimplementedError\u001b[0m: Exception encountered when calling layer 'string_lookup_1' (type StringLookup).\n\n{{function_node __wrapped__Cast_device_/job:localhost/replica:0/task:0/device:CPU:0}} Cast float to string is not supported [Op:Cast]\n\nCall arguments received by layer 'string_lookup_1' (type StringLookup):\n  â€¢ inputs=tf.Tensor(shape=(), dtype=float32)"
          ]
        }
      ],
      "source": [
        "K_Factor = (5, 10, 20, 40, 60, 80, 100) # Change this according to desired values\n",
        "\n",
        "fact_topK = FactorizedTopK(\n",
        "  candidates=movies_df.MovieID.map(candidate_model),\n",
        "  ks = K_Factor\n",
        ")\n",
        "\n",
        "loss_fn = lambda: tfrs.tasks.Retrieval(metrics=fact_topK)\n",
        "metrics_fn = lambda: [fact_topK]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7GF8WOALvMm"
      },
      "source": [
        "# Training Federated Reconstruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUl-OhGLLuvG"
      },
      "outputs": [],
      "source": [
        "# Federated Training Process\n",
        "training_process = tff.learning.reconstruction.build_training_process(\n",
        "    model_fn=model_fn,\n",
        "    loss_fn=loss_fn,\n",
        "    metrics_fn=metrics_fn,\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.Adagrad(learning_rate=1),\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.Adagrad(learning_rate=0.5),\n",
        "    reconstruction_optimizer_fn=lambda: tf.keras.optimizers.Adagrad(learning_rate=0.1))\n",
        "\n",
        "# Federated Evaluation Process\n",
        "evaluation_computation = tff.learning.reconstruction.build_federated_evaluation(\n",
        "    model_fn,\n",
        "    loss_fn=loss_fn,\n",
        "    metrics_fn=metrics_fn,\n",
        "    reconstruction_optimizer_fn=lambda: tf.keras.optimizers.Adagrad(learning_rate=0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqTId60vMwMr"
      },
      "outputs": [],
      "source": [
        "# Initialize Model State\n",
        "# This model has yet to be trained so its states are randomly initialized\n",
        "state = training_process.initialize()\n",
        "print(state.model)\n",
        "print('Item variables shape:', state.model.trainable[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vh5hCnhWNAFP"
      },
      "outputs": [],
      "source": [
        "# Testing evaluation pipelines on evaluating initial state model\n",
        "eval_metrics = evaluation_computation(state.model, tf_val_datasets)\n",
        "print('Initial Eval:', eval_metrics['eval'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MX6ViGthNqbV"
      },
      "outputs": [],
      "source": [
        "# Training loops\n",
        "NUM_EPOCHS = 50\n",
        "NUM_CLIENT_PER_EPOCH = 100\n",
        "\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "\n",
        "# This may take a couple minutes to run.\n",
        "print(f\"Training with {NUM_CLIENT_PER_EPOCH} clients per sample of Epoch. Total number of epochs: {NUM_EPOCHS}\")\n",
        "for i in range(NUM_EPOCHS):\n",
        "  federated_train_data = np.random.choice(tf_train_datasets, size=NUM_CLIENT_PER_EPOCH, replace=False).tolist()\n",
        "  state, metrics = training_process.next(state, federated_train_data)\n",
        "  print(f'Epoch {i+1}/{NUM_EPOCHS}:', metrics['train'])\n",
        "  train_losses.append(metrics['train']['loss'])\n",
        "  train_accs.append(metrics['train']['rating_accuracy'])\n",
        "\n",
        "\n",
        "eval_metrics = evaluation_computation(state.model, tf_val_datasets)\n",
        "print('Final Eval:', eval_metrics['eval'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxY5BSIBQWp-"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-pIPoLsQSPk"
      },
      "outputs": [],
      "source": [
        "# Evaluating model\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "fig.suptitle('Federated model evaluation')\n",
        "fig.set_size_inches(15, 6)\n",
        "\n",
        "ax[0].plot(range(NUM_EPOCHS), train_losses)\n",
        "ax[0].set_ylabel('Train Loss')\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_title('Train Loss')\n",
        "\n",
        "ax[1].plot(range(NUM_EPOCHS), train_accs)\n",
        "ax[1].set_ylabel('Train Accuracy')\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_title('Train Accuracy')\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
